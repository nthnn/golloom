/*
 * Copyright 2025 Nathanne Isip
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */

package golloom

import (
	"context"
	"net/url"
	"time"
)

// Message represents a single message in a conversation, either sent by the user or generated by the assistant.
// It encapsulates the sender's role, the textual content of the message, and optional metadata such as images and tool call details.
type Message struct {
	// Role specifies the origin of the message, for example, "user" or "assistant".
	Role string `json:"role"`
	// Content holds the main textual content of the message.
	Content string `json:"content"`

	// Images contains an optional list of image URLs associated with the message.
	Images []string `json:"images,omitempty"`
	// ToolCalls holds an optional list of tool call details, where each tool call is represented as a map.
	ToolCalls []map[string]interface{} `json:"tool_calls,omitempty"`
}

// Chat encapsulates the data required to initiate a conversation with the language model.
// It includes the target model, a history of exchanged messages, and various optional configuration settings.
type Chat struct {
	// Model indicates which language model should process the chat request.
	Model string `json:"model"`
	// Messages is a slice of Message structs representing the conversation history.
	Messages []Message `json:"messages"`

	// Format is an optional parameter that defines the desired format of the response.
	// It can be specified as a string or a more complex structure.
	Format interface{} `json:"format,omitempty"`
	// Options provides additional optional settings as key-value pairs for the chat session.
	Options map[string]interface{} `json:"options,omitempty"`
	// Tools lists optional tools (as maps) that may be utilized during the conversation.
	Tools []map[string]interface{} `json:"tools,omitempty"`

	// Stream is an optional flag that, when set to true, requests the server to stream the response.
	Stream *bool `json:"stream,omitempty"`
	// KeepAlive specifies an optional mechanism or duration to maintain the connection during streaming.
	KeepAlive string `json:"keep_alive,omitempty"`
}

// ModelResponse represents the response returned by the language model after processing a chat request.
// It includes metadata about the response as well as the message generated by the model.
type ModelResponse struct {
	// Model specifies which language model generated the response.
	Model string `json:"model"`
	// CreatedAt records the timestamp when the response was created.
	CreatedAt time.Time `json:"created_at"`
	// Message contains the actual message content generated by the model.
	Message Message `json:"message"`
	// Done indicates whether the model has completed generating the response.
	Done bool `json:"done"`
	// DoneReason provides an optional explanation of why the response generation has stopped.
	DoneReason string `json:"done_reason,omitempty"`
	// TotalDuration measures the total time taken to generate the response (in milliseconds or a similar unit).
	TotalDuration int64 `json:"total_duration,omitempty"`
	// LoadDuration represents the time taken to load the model or necessary data for generation.
	LoadDuration int64 `json:"load_duration,omitempty"`
	// PromptEvalCount counts the number of prompt evaluations performed during response generation.
	PromptEvalCount int `json:"prompt_eval_count,omitempty"`
	// PromptEvalDuration captures the total time spent evaluating the prompt.
	PromptEvalDuration int64 `json:"prompt_eval_duration,omitempty"`
	// EvalCount indicates the number of evaluations executed during the generation process.
	EvalCount int `json:"eval_count,omitempty"`
	// EvalDuration records the time spent in the evaluation phase of generating the response.
	EvalDuration int64 `json:"eval_duration,omitempty"`
}

// Chat sends a chat request to the Ollama server and retrieves the model's response.
// It constructs the full API endpoint URL and delegates the HTTP POST request to the sendChatRequest helper function.
// Parameters:
//   - ctx: A context for controlling cancellation and timeouts for the request.
//   - req: A pointer to a Chat struct containing the conversation history and optional configuration.
//
// Returns:
//   - A pointer to a ModelResponse struct that encapsulates the response and related metadata.
//   - An error if the HTTP request or response decoding fails.
func (c *Client) Chat(
	ctx context.Context,
	req *Chat,
) (*ModelResponse, error) {
	rel := &url.URL{Path: "/api/chat"}
	u := c.BaseURL.ResolveReference(rel)

	return c.sendChatRequest(
		ctx,
		"POST",
		u.String(),
		req,
	)
}
